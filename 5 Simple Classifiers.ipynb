{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"application_train.csv\")\n",
    "df_test = pd.read_csv(\"application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in df_train:\n",
    "    if df_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(df_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(df_train[col])\n",
    "            # Transform both training and testing data\n",
    "            df_train[col] = le.transform(df_train[col])\n",
    "            df_test[col] = le.transform(df_test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 243)\n",
      "Testing Features shape:  (48744, 239)\n",
      "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR',\n",
      "       'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',\n",
      "       'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
      "       ...\n",
      "       'HOUSETYPE_MODE_terraced house', 'WALLSMATERIAL_MODE_Block',\n",
      "       'WALLSMATERIAL_MODE_Mixed', 'WALLSMATERIAL_MODE_Monolithic',\n",
      "       'WALLSMATERIAL_MODE_Others', 'WALLSMATERIAL_MODE_Panel',\n",
      "       'WALLSMATERIAL_MODE_Stone, brick', 'WALLSMATERIAL_MODE_Wooden',\n",
      "       'EMERGENCYSTATE_MODE_No', 'EMERGENCYSTATE_MODE_Yes'],\n",
      "      dtype='object', length=243)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)\n",
    "list1=df_train.columns\n",
    "\n",
    "print('Training Features shape: ', df_train.shape)\n",
    "print('Testing Features shape: ', df_test.shape)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 243 columns.\n",
      "There are 61 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['COMMONAREA_MODE', 'COMMONAREA_MEDI', 'COMMONAREA_AVG',\n",
       "       'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_AVG',\n",
       "       'NONLIVINGAPARTMENTS_MEDI', 'LIVINGAPARTMENTS_MEDI',\n",
       "       'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MODE', 'FLOORSMIN_MEDI',\n",
       "       'FLOORSMIN_MODE', 'FLOORSMIN_AVG', 'YEARS_BUILD_AVG',\n",
       "       'YEARS_BUILD_MODE', 'YEARS_BUILD_MEDI', 'OWN_CAR_AGE', 'LANDAREA_MEDI',\n",
       "       'LANDAREA_MODE', 'LANDAREA_AVG', 'BASEMENTAREA_MODE',\n",
       "       'BASEMENTAREA_AVG', 'BASEMENTAREA_MEDI', 'EXT_SOURCE_1',\n",
       "       'NONLIVINGAREA_MODE', 'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MEDI',\n",
       "       'ELEVATORS_MEDI', 'ELEVATORS_MODE', 'ELEVATORS_AVG', 'APARTMENTS_AVG',\n",
       "       'APARTMENTS_MEDI', 'APARTMENTS_MODE', 'ENTRANCES_AVG', 'ENTRANCES_MEDI',\n",
       "       'ENTRANCES_MODE', 'LIVINGAREA_MEDI', 'LIVINGAREA_AVG',\n",
       "       'LIVINGAREA_MODE', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG',\n",
       "       'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',\n",
       "       'YEARS_BEGINEXPLUATATION_MODE', 'TOTALAREA_MODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(df_train)\n",
    "missing_values.index[:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "newX=df_train.drop(missing_values.index[:15],axis=1)\n",
    "newX.fillna(-1000, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split( newX.drop('TARGET',axis=1) , df_train['TARGET'] , test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UTSAV\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regr. Score =  0.9194998617953596\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X_train, y_train)\n",
    "print(\"Logistic Regr. Score = \", clf1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC 0.6286\n",
      "Test AUC 0.6317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_train_predicted = clf1.predict_proba(X_train)[:,1]\n",
    "y_test_predicted = clf1.predict_proba(X_test)[:,1]\n",
    "print(\"Train AUC %.4f\"%roc_auc_score(y_train,y_train_predicted))\n",
    "print(\"Test AUC %.4f\"%roc_auc_score(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9192\n",
      "Test Accuracy 0.9195\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf1.predict(X_train)\n",
    "y_test_pred = clf1.predict(X_test)\n",
    "print(\"Train Accuracy %.4f\" % accuracy_score(y_train,y_train_pred))\n",
    "print(\"Test Accuracy %.4f\" % accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56552     2]\n",
      " [ 4949     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confmat=confusion_matrix(y_test,y_test_pred)\n",
    "print(confmat)\n",
    "# plt.imshow(confmat, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Score =  0.919662455489976\n"
     ]
    }
   ],
   "source": [
    "clf2 = XGBClassifier()\n",
    "clf2.fit(X_train, y_train)\n",
    "print(\"XGBoost Score = \", clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC 0.7581\n",
      "Test AUC 0.7504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_train_predicted = clf2.predict_proba(X_train)[:,1]\n",
    "y_test_predicted = clf2.predict_proba(X_test)[:,1]\n",
    "print(\"Train AUC %.4f\"%roc_auc_score(y_train,y_train_predicted))\n",
    "print(\"Test AUC %.4f\"%roc_auc_score(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9194\n",
      "Test Accuracy 0.9197\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf2.predict(X_train)\n",
    "y_test_pred = clf2.predict(X_test)\n",
    "print(\"Train Accuracy %.4f\" % accuracy_score(y_train,y_train_pred))\n",
    "print(\"Test Accuracy %.4f\" % accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56543    11]\n",
      " [ 4930    19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confmat=confusion_matrix(y_test,y_test_pred)\n",
    "print(confmat)\n",
    "# plt.imshow(confmat, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score =  0.9139879355478595\n"
     ]
    }
   ],
   "source": [
    "clf3 = KNeighborsClassifier()\n",
    "clf3.fit(X_train, y_train)\n",
    "print(\"KNN Score = \", clf3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC 0.8880\n",
      "Test AUC 0.5387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_train_predicted = clf3.predict_proba(X_train)[:,1]\n",
    "y_test_predicted = clf3.predict_proba(X_test)[:,1]\n",
    "print(\"Train AUC %.4f\"%roc_auc_score(y_train,y_train_predicted))\n",
    "print(\"Test AUC %.4f\"%roc_auc_score(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9208\n",
      "Test Accuracy 0.9140\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf3.predict(X_train)\n",
    "y_test_pred = clf3.predict(X_test)\n",
    "print(\"Train Accuracy %.4f\" % accuracy_score(y_train,y_train_pred))\n",
    "print(\"Test Accuracy %.4f\" % accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56153   401]\n",
      " [ 4889    60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confmat=confusion_matrix(y_test,y_test_pred)\n",
    "print(confmat)\n",
    "# plt.imshow(confmat, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UTSAV\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Score =  0.9186543745833536\n"
     ]
    }
   ],
   "source": [
    "clf4 = RandomForestClassifier()\n",
    "clf4.fit(X_train, y_train)\n",
    "print(\"Random Forest Score = \", clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC 0.9998\n",
      "Test AUC 0.6184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_train_predicted = clf4.predict_proba(X_train)[:,1]\n",
    "y_test_predicted = clf4.predict_proba(X_test)[:,1]\n",
    "print(\"Train AUC %.4f\"%roc_auc_score(y_train,y_train_predicted))\n",
    "print(\"Test AUC %.4f\"%roc_auc_score(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9208\n",
      "Test Accuracy 0.9140\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf3.predict(X_train)\n",
    "y_test_pred = clf3.predict(X_test)\n",
    "print(\"Train Accuracy %.4f\" % accuracy_score(y_train,y_train_pred))\n",
    "print(\"Test Accuracy %.4f\" % accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56153   401]\n",
      " [ 4889    60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confmat=confusion_matrix(y_test,y_test_pred)\n",
    "print(confmat)\n",
    "# plt.imshow(confmat, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
